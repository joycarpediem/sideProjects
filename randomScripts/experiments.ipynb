{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv('../.env')\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Species list for a location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"x-ebirdapitoken\": os.getenv(\"x-ebirdapitoken\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locId=\"L6183959\"\n",
    "# url= f\"https://api.ebird.org/v2/ref/hotspot/info/{locId}\"\n",
    "url=f\"https://api.ebird.org/v2/product/spplist/{locId}\"\n",
    "birdlistforloc=requests.get(url, headers=headers).json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://api.ebird.org/v2/ref/taxonomy/ebird?fmt=json\"\n",
    "allBirds=requests.get(url, headers=headers).json()\n",
    "allBirdsdf=pd.json_normalize(allBirds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allBirdsdf= allBirdsdf[allBirdsdf['category'] == \"species\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_df = pd.DataFrame({'speciesCode': birdlistforloc}).merge(allBirdsdf[[\"comName\",\"speciesCode\"]], on='speciesCode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reading text from images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the Tesseract executable (change it according to your installation)\n",
    "pytesseract.pytesseract.tesseract_cmd = '/opt/homebrew/bin/tesseract'\n",
    "\n",
    "# Open the image file\n",
    "image = Image.open('/Users/joy/akvo/garbage/ss/Deden.png')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "English (eng)\n",
    "Spanish (spa)\n",
    "French (fra)\n",
    "German (deu)\n",
    "Italian (ita)\n",
    "Portuguese (por)\n",
    "Dutch (nld)\n",
    "Russian (rus)\n",
    "Chinese (chi_sim for Simplified Chinese and chi_tra for Traditional Chinese)\n",
    "Japanese (jpn)\n",
    "Korean (kor)\n",
    "Arabic (ara)\n",
    "Hindi (hin)\n",
    "Bengali (ben)\n",
    "Tamil (tam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the image to grayscale\n",
    "image = image.convert('L')\n",
    "languages = ['eng']\n",
    "# Use Tesseract to extract text from the image\n",
    "extracted_text = \"\"\n",
    "\n",
    "# Iterate over each language and extract text\n",
    "for lang in languages:\n",
    "    text = pytesseract.image_to_string(image, lang=lang)\n",
    "    extracted_text += text + \"\\n\"\n",
    "\n",
    "# Print the extracted text\n",
    "print(extracted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuweather API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "from urllib.parse import quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apiKey=os.getenv(\"accu_apiKey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latLong=quote('28.583343,77.174635')\n",
    "locationKey= requests.get(f'http://dataservice.accuweather.com/locations/v1/cities/geoposition/search?apikey={apiKey}&q={latLong}').json()['Key']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast=requests.get(f'http://dataservice.accuweather.com/forecasts/v1/daily/5day/{locationKey}?apikey={apiKey}&details=true&metric=true').json()\n",
    "forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = [user['Day'][\"Rain\"][\"Value\"] for user in forecast[\"DailyForecasts\"]]\n",
    "dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bulk download youtube video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Set up the YouTube Data API client\n",
    "api_key = os.getenv(\"youtube_apikey\")  # Replace with your actual API key\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "# Define the search query\n",
    "search_query = \"Pratham Alo ( প্রথম আলো ) by Sunil Gangopadhyay ; Part\"\n",
    "channel_id = \"UCK-g7XJuUzonElW_pMJVAsw\"  # Replace with the actual channel ID\n",
    "output_directory = os.path.expanduser(\"~/Downloads/Series/PP\")\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_results = 100\n",
    "next_page_token = None\n",
    "\n",
    "# Create empty lists to store video links, titles, and channel IDs\n",
    "video_links = []\n",
    "video_titles = []\n",
    "channel_ids = []\n",
    "\n",
    "# Retrieve all available search results\n",
    "while True:\n",
    "    # Call the search.list method to retrieve the search results\n",
    "    search_response = youtube.search().list(\n",
    "        q=search_query,\n",
    "        channelId=channel_id,\n",
    "        part='id,snippet',\n",
    "        maxResults=max_results,\n",
    "        pageToken=next_page_token\n",
    "    ).execute()\n",
    "\n",
    "    # Iterate over the search results and add the video links, titles, and channel IDs to the lists\n",
    "    for item in search_response['items']:\n",
    "        if 'videoId' in item['id']:\n",
    "            video_id = item['id']['videoId']\n",
    "            video_link = f\"https://www.youtube.com/watch?v={video_id}\"\n",
    "            video_title = item['snippet']['title']\n",
    "            channel_id = item['snippet']['channelId']\n",
    "            video_links.append(video_link)\n",
    "            video_titles.append(video_title)\n",
    "            channel_ids.append(channel_id)\n",
    "\n",
    "    # Check if there are more pages of results\n",
    "    if 'nextPageToken' in search_response:\n",
    "        next_page_token = search_response['nextPageToken']\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Create a DataFrame from the lists of video links, titles, and channel IDs\n",
    "df_videos = pd.DataFrame({\n",
    "    'Video Link': video_links,\n",
    "    'Video Title': video_titles,\n",
    "    'Channel ID': channel_ids\n",
    "})\n",
    "\n",
    "# Filter the DataFrame to include only videos from the specified channel ID\n",
    "filtered_df = df_videos[df_videos['Channel ID'] == channel_id]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Filter the DataFrame to include only rows where the video title matches the criteria . In this case we check if a number above 47 is part of the title\n",
    "filtered_df = df_videos[\n",
    "    df_videos['Video Title'].str.contains(\"Pratham Alo\") &\n",
    "    df_videos['Video Title'].str.contains(r\"\\b(?:4[8-9]|[5-9]\\d+)\\b\", regex=True)\n",
    "]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for index, row in filtered_df.iterrows():\n",
    "    numbers = re.findall(r'\\b(?:4[8-9]|[5-9]\\d+)\\b', row['Video Title'])  # Extract numbers above 48 using regex pattern\n",
    "\n",
    "    # Check if numbers are found in the title\n",
    "    if numbers:\n",
    "        number = numbers[0]  # Get the first number\n",
    "        filename = f\"PA-{number}.mp3\"\n",
    "        command = [\n",
    "            \"yt-dlp\",\n",
    "            \"-x\",\n",
    "            \"--audio-format\",\n",
    "            \"mp3\",\n",
    "            \"--output\",\n",
    "            f\"{output_directory}/{filename}\",\n",
    "            row['Video Link']\n",
    "        ]\n",
    "        subprocess.run(command) \n",
    "# TEST WITH ONE RECORD\n",
    "# first_row = filtered_df.iloc[0]\n",
    "\n",
    "# # Extract the number from the video title\n",
    "# numbers = re.findall(r'\\b(?:4[8-9]|[5-9]\\d+)\\b', first_row['Video Title'])\n",
    "# if numbers:\n",
    "#     number = numbers[0]\n",
    "#     filename = f\"PA-{number}.mp3\"\n",
    "#     command = [\n",
    "#         \"yt-dlp\",\n",
    "#         \"-x\",\n",
    "#         \"--audio-format\",\n",
    "#         \"mp3\",\n",
    "#         \"--output\",\n",
    "#         f\"{output_directory}/{filename}\",\n",
    "#         first_row['Video Link']\n",
    "#     ]\n",
    "#     subprocess.run(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text to speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtts import gTTS\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_advisory = \"\"\"\n",
    "Weather Advisory - Next 5 Days:\n",
    "\n",
    "Day 1:\n",
    "- Min Temperature: 15°C\n",
    "- Max Temperature: 27°C\n",
    "- Rainfall Forecast: Light showers expected.\n",
    "\n",
    "Day 2:\n",
    "- Min Temperature: 14°C\n",
    "- Max Temperature: 26°C\n",
    "- Rainfall Forecast: Mostly clear skies, no rainfall expected.\n",
    "\n",
    "Day 3:\n",
    "- Min Temperature: 16°C\n",
    "- Max Temperature: 28°C\n",
    "- Rainfall Forecast: Heavy rain expected in the evening.\n",
    "\n",
    "Day 4:\n",
    "- Min Temperature: 17°C\n",
    "- Max Temperature: 29°C\n",
    "- Rainfall Forecast: Partly cloudy, chance of scattered showers.\n",
    "\n",
    "Day 5:\n",
    "- Min Temperature: 18°C\n",
    "- Max Temperature: 30°C\n",
    "- Rainfall Forecast: Thunderstorms likely in the afternoon.\n",
    "\n",
    "Please stay prepared and take necessary precautions for any weather changes.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_speech(text, lang='en'):\n",
    "    tts = gTTS(text=text, lang=lang)\n",
    "    tts.save(\"output.mp3\")  # Save the speech to a file in the current directory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    text_to_speech(weather_advisory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jugalbandi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from urllib.parse import urlencode, quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'uuid_number': '50296d62-2df8-11ee-ba3a-515343c435e8',\n",
    "    'query_string': 'Which province grows the most rice'\n",
    "    \n",
    "}\n",
    "params=urlencode(params, quote_via=quote)\n",
    "url=f'https://api.jugalbandi.ai/query-with-gptindex?{params}'\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content= requests.get(url).json()\n",
    "content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Kenya data archieve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyreadstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spss_file_path=\"/Users/joy/Downloads/SPSS/KEPR8ASV/KEPR8AFL.SAV\"\n",
    "# Read the SPSS file into a pandas DataFrame and get the metadata\n",
    "df, meta = pyreadstat.read_sav(spss_file_path)\n",
    "\n",
    "# Access the variable labels (column descriptions) from the metadata\n",
    "variable_labels = meta.column_labels\n",
    "\n",
    "# Print the variable labels for each column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.columns = [variable_labels.get(col, col) for col in df.columns]\n",
    "\n",
    "# Now, the DataFrame has column names as the variable labels (descriptions)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bulk rename files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "folder_path=\"/Users/joy/Downloads/series/PP\"\n",
    "# search_pattern = r'পর্ব\\s*-*\\s*([০-৯]+)'\n",
    "search_pattern =r'^M-(.*)'\n",
    "new_file_prefix=\"PP-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only for bengali numbers\n",
    "def bengali_to_int(bengali_numeral):\n",
    "    bengali_digits = {\n",
    "        '০': 0, '১': 1, '২': 2, '৩': 3, '৪': 4,\n",
    "        '৫': 5, '৬': 6, '৭': 7, '৮': 8, '৯': 9\n",
    "    }\n",
    "    return int(''.join(str(bengali_digits[digit]) for digit in bengali_numeral))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bulk_rename(directory, search_pattern):\n",
    "    for filename in os.listdir(directory):\n",
    "        old_path = os.path.join(directory, filename)\n",
    "        if os.path.isfile(old_path):\n",
    "            match = re.search(search_pattern, filename)\n",
    "            if match:\n",
    "                part_number = match.group(1)\n",
    "                #only for bengali numbers\n",
    "                # part_number = bengali_to_int(part_number)\n",
    "                new_filename = f'{new_file_prefix}{part_number}.mp3'\n",
    "                new_path = os.path.join(directory, new_filename)\n",
    "                os.rename(old_path, new_path)\n",
    "                print(f'Renamed: {filename} -> {new_filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    directory_path = input(folder_path)\n",
    "    bulk_rename(directory_path, search_pattern)\n",
    "    print(\"Bulk renaming complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = re.search(r'^M-(.*)', \"M-127.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match.group(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Develop cascade from API output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"/Users/joy/akvo/projects/RTMIS/dsl_admin.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entities = pd.DataFrame(columns=['County', 'Sub-county', 'Ward'])\n",
    "lst = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    entity_id = row['id']\n",
    "    parent_id = row['parentid']\n",
    "    entity_name = row['name']\n",
    "    entity_level = row['level']\n",
    "\n",
    "    if entity_level == 4:\n",
    "        # print(entity_level,entity_name)\n",
    "        level3_parent = df[df['id'] == parent_id]\n",
    "        level3_parent_name=level3_parent['name'].values[0]\n",
    "        level3_parent_parent_id=level3_parent['parentid'].values[0]\n",
    "        level2=df[df['id'] == level3_parent_parent_id]\n",
    "        level2_name=level2['name'].values[0]\n",
    "        lst.append({'County':level2_name, 'Sub-county':level3_parent_name, 'Ward':entity_name})\n",
    "        # entities=pd.concat([entities,pd.DataFrame({'County':level2_name, 'Sub-county':level3_parent_name, 'Ward':entity_name})])\n",
    "        \n",
    "\n",
    "# entities = entities.sort_values(by=['County'])\n",
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ext=pd.DataFrame(lst)\n",
    "df_ext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "df_ext['Ward'] = df_ext['Ward'].apply(lambda x: re.sub(r'\\s*\\(.*\\)', '', x))\n",
    "df_ext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ext.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ext=df_ext.sort_values(by=['County'],ignore_index=True)\n",
    "df_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ext.to_csv(\"/Users/joy/akvo/projects/RTMIS/dsl_admin_readable.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Farmer land size distribution check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#parameters\n",
    "confidence_level=0.95\n",
    "p_value_threshold=0.01\n",
    "variable='cal_actual_income'\n",
    "variable_title= 'Actual Income'\n",
    "\n",
    "\n",
    "# Path to the folder containing Excel files\n",
    "folder_path = \"data/SHF/PDC/farmfit_portal_files\"\n",
    "\n",
    "# List to store results\n",
    "results = []\n",
    "\n",
    "# Loop through all Excel files in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if not file_name.startswith(\"~\") and file_name.endswith(\".xlsx\"):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "        # Load data from the \"Cleaned Data\" sheet\n",
    "        df = pd.read_excel(file_path, sheet_name=\"Cleaned Data\")\n",
    "        # Check if variable column is available in the dataset\n",
    "        if variable not in df.columns:\n",
    "            # print(f\"Dataset '{file_name}' does not have {variable} column. Skipping.\")\n",
    "            continue\n",
    "        df = df.dropna(subset=[variable])\n",
    "        # df = df[df[variable] > 0]  \n",
    "        # Check if \"f_unit_land\" is \"hectares\" and adjust variable accordingly\n",
    "        # df.loc[df[\"f_unit_land\"] == \"hectares\", variable] *= 2.47\n",
    "        # df = df[df[variable] <= 9000]\n",
    "        # Calculate statistics\n",
    "        mean_size = df[variable].mean()\n",
    "        n = len(df[variable])\n",
    "        confidence_interval = stats.t.interval(confidence_level, n - 1, loc=mean_size, scale=stats.sem(df[variable]))\n",
    "         # Compute the difference between the upper and lower confidence interval\n",
    "        confidence_interval_diff = confidence_interval[1] - confidence_interval[0]\n",
    "        iqr = df[variable].quantile(0.75) - df[variable].quantile(0.25)\n",
    "        iqr_lower_limit = df[variable].quantile(0.25) - 1.5 * iqr\n",
    "        iqr_upper_limit = df[variable].quantile(0.75) + 1.5 * iqr\n",
    "        std_dev = df[variable].std()\n",
    "        lower_bound = mean_size - 2 * std_dev\n",
    "        upper_bound = mean_size + 2 * std_dev\n",
    "        \n",
    "        # Fit different distributions and find the best fit\n",
    "        # Expanded list of distributions\n",
    "        distributions = [\n",
    "            stats.norm,        # Normal distribution\n",
    "            stats.expon,       # Exponential distribution\n",
    "            stats.gamma,       # Gamma distribution\n",
    "            stats.lognorm,     # Lognormal distribution\n",
    "            stats.weibull_min, # Weibull distribution\n",
    "            stats.pareto,      # Pareto distribution\n",
    "            stats.beta,        # Beta distribution\n",
    "            stats.exponweib,   # Exponentiated Weibull distribution\n",
    "            stats.logistic,    # Logistic distribution\n",
    "            stats.t,           # Student's t-distribution\n",
    "            # Add more distributions as needed\n",
    "        ]\n",
    "\n",
    "        \n",
    "        best_fit = None\n",
    "        best_fit_name = ''\n",
    "        best_fit_params = ()\n",
    "        best_fit_p_value = np.inf\n",
    "        \n",
    "        data = df[variable]\n",
    "        \n",
    "        for distribution in distributions:\n",
    "            params = distribution.fit(data)\n",
    "            _, p_value = stats.kstest(data, distribution.name, args=params)\n",
    "            \n",
    "            if p_value < best_fit_p_value:\n",
    "                best_fit = distribution\n",
    "                best_fit_name = distribution.name\n",
    "                best_fit_params = params\n",
    "                best_fit_p_value = p_value\n",
    "                fit_is_good = \"Good\" if best_fit_p_value > p_value_threshold else \"Not Good\"\n",
    "        \n",
    "        # Append results to the list\n",
    "        results.append({\n",
    "            \"File Name\": file_name,\n",
    "            f\"Mean of {variable_title}\": mean_size,\n",
    "            \"Confidence Interval\": confidence_interval,\n",
    "            \"CI range\" : confidence_interval_diff,\n",
    "            \"IQR\": iqr,\n",
    "            \"IQR Lower Limit\": iqr_lower_limit,\n",
    "            \"IQR Upper Limit\" : iqr_upper_limit,\n",
    "            \"2SD Upper Limit\": upper_bound,\n",
    "            \"2SD Lower Limit\": lower_bound,\n",
    "            \"Best Fit Distribution\": best_fit_name,\n",
    "            # \"Best Fit Params\": best_fit_params,\n",
    "            \"Best Fit P Value\": best_fit_p_value,\n",
    "            \"Fit Quality\": fit_is_good\n",
    "        })\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        # plt.hist(df[variable], bins=20, edgecolor=\"black\")\n",
    "        # sns.kdeplot(data, shade=True)\n",
    "         # Overlay the PDF curve of the best fit distribution\n",
    "        x = np.linspace(min(df[variable]), max(df[variable]), 1000)\n",
    "        best_fit_distribution = getattr(stats, best_fit_name)\n",
    "        pdf = best_fit_distribution.pdf(x, *best_fit_params)\n",
    "        \n",
    "        # Plot PDF of best fit distribution on a separate y-axis\n",
    "        fig, ax1 = plt.subplots()\n",
    "        ax1.set_xlabel(variable_title)\n",
    "        ax1.set_ylabel(\"Density (KDE)\", color='tab:blue')\n",
    "        sns.kdeplot(df[variable], shade=True, ax=ax1, color='tab:blue')\n",
    "        ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "        # ax2 = ax1.twinx()\n",
    "        # ax2.set_ylabel(f\"Probability Density Function (PDF) {best_fit_name}\", color='tab:red')\n",
    "        # sns.kdeplot(df[variable], shade=True, ax=ax1, color='tab:green')\n",
    "        # # ax2.plot(x, pdf, color='tab:red', label=best_fit_name + \" PDF\")\n",
    "        # ax2.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "\n",
    "        plt.title(f\"{variable_title} Distribution for {file_name}\")\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Print the results\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns  # Import seaborn for color mapping\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the audio file\n",
    "audio_file = \"data/audio/nenjinile1-10.wav\"\n",
    "y, sr = librosa.load(audio_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate the spectrogram\n",
    "D = librosa.amplitude_to_db(librosa.stft(y), ref=np.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a frequency axis for the spectrogram\n",
    "frequencies = librosa.core.fft_frequencies(sr=sr)\n",
    "dominant_frequencies = frequencies[np.argmax(D, axis=0)]\n",
    "\n",
    "# Define a mapping of frequencies to notes\n",
    "def frequency_to_note(frequency):\n",
    "    note_names = [\"C\", \"C#\", \"D\", \"D#\", \"E\", \"F\", \"F#\", \"G\", \"G#\", \"A\", \"A#\", \"B\"]\n",
    "    A4_frequency = 440.0  # Frequency of A4 note in Hz\n",
    "    note_index = round(12 * np.log2(frequency / A4_frequency)) % 12\n",
    "    octave = round(np.log2(frequency / A4_frequency))\n",
    "    return f\"{note_names[note_index]}{octave}\", octave\n",
    "\n",
    "# Get note labels and octaves for the frequency axis\n",
    "# note_labels, octaves = zip(*[frequency_to_note(f) for f in frequencies])\n",
    "note_labels = []\n",
    "octaves = []\n",
    "\n",
    "for f in dominant_frequencies:\n",
    "    try:\n",
    "        note, octave = frequency_to_note(f)\n",
    "    except OverflowError:\n",
    "        # Handle the error here by setting default values\n",
    "        note = \"Unknown\"\n",
    "        octave = 0\n",
    "    \n",
    "    note_labels.append(note)\n",
    "    octaves.append(octave)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a color map for octaves using seaborn\n",
    "octave_colors = sns.color_palette(\"husl\", n_colors=len(set(octaves)))\n",
    "\n",
    "# Display the spectrogram with colored note labels\n",
    "plt.figure(figsize=(12, 6))\n",
    "librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='log')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Spectrogram with Colored Note Labels')\n",
    "\n",
    "# Add note labels to the frequency axis with colors based on octaves\n",
    "for i, label in enumerate(note_labels):\n",
    "    plt.text(-0.5, i, label, fontsize=8, ha=\"right\", va=\"center\", color=octave_colors[octaves[i]])\n",
    "\n",
    "# Set the color map for the y-axis to match octave colors\n",
    "plt.yticks(range(0, len(frequencies), 12), ['C0', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8'])\n",
    "plt.gca().yaxis.set_tick_params(labelcolor=octave_colors)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the spectrogram\n",
    "D = np.abs(librosa.stft(y))\n",
    "\n",
    "# Get the number of frames\n",
    "num_frames = D.shape[1]\n",
    "\n",
    "# Compute the frame length in seconds\n",
    "frame_length = len(y) / num_frames / sr\n",
    "\n",
    "# Create a time axis\n",
    "times = np.arange(0, num_frames) * frame_length\n",
    "\n",
    "# Get the frequency bins corresponding to each time frame\n",
    "frequencies = librosa.core.fft_frequencies(sr=sr)\n",
    "\n",
    "# Find the frequency bin with the highest magnitude for each time frame\n",
    "dominant_frequency_bins = []\n",
    "\n",
    "for frame in np.transpose(D):  # Transpose D to iterate over frames\n",
    "    if np.all(np.isinf(frame)):\n",
    "        # Handle the case of all values being infinity\n",
    "        dominant_frequency_bins.append(0)  # Assign a placeholder value\n",
    "    else:\n",
    "        dominant_frequency_bins.append(np.argmax(frame))\n",
    "\n",
    "# Create lists to store the data for plotting\n",
    "plot_times = []\n",
    "plot_frequencies = []\n",
    "note_labels = []\n",
    "\n",
    "for i, freq_bin in enumerate(dominant_frequency_bins):\n",
    "    if freq_bin == 0:\n",
    "        # Handle the placeholder value for infinite frames\n",
    "        continue\n",
    "    \n",
    "    # Convert frequency bin to Hertz\n",
    "    dominant_frequency_hz = frequencies[freq_bin]\n",
    "    \n",
    "    # Convert Hertz to note name (with error handling)\n",
    "    try:\n",
    "        note_names = [\"C\", \"C#\", \"D\", \"D#\", \"E\", \"F\", \"F#\", \"G\", \"G#\", \"A\", \"A#\", \"B\"]\n",
    "        A4_frequency = 440.0  # Frequency of A4 note in Hz\n",
    "        note_index = round(12 * np.log2(dominant_frequency_hz / A4_frequency)) % 12\n",
    "        octave = round(np.log2(dominant_frequency_hz / A4_frequency))\n",
    "        note_label = f\"{note_names[note_index]}{octave}\"\n",
    "    except (ValueError, OverflowError, ZeroDivisionError):\n",
    "        note_label = \"N/A\"  # Set a placeholder for problematic cases\n",
    "    \n",
    "    # Store data for plotting\n",
    "    plot_times.append(times[i])\n",
    "    plot_frequencies.append(dominant_frequency_hz)\n",
    "    note_labels.append(note_label)\n",
    "\n",
    "# Create a plot showing the dominant frequency at each time frame with note labels\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(plot_times, plot_frequencies, 'bo-', label='Dominant Frequency')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Frequency (Hz)')\n",
    "plt.title('Dominant Frequency Plot from Spectrogram with Note Labels')\n",
    "plt.grid()\n",
    "\n",
    "# Add note labels to the plot\n",
    "for t, f, label in zip(plot_times, plot_frequencies, note_labels):\n",
    "    plt.text(t, f, label, fontsize=8, ha=\"right\", va=\"bottom\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the audio file\n",
    "audio_file = \"data/audio/nenjinile1-10.wav\"\n",
    "y, sr = librosa.load(audio_file)\n",
    "\n",
    "# Compute the spectrogram\n",
    "D = np.abs(librosa.stft(y))\n",
    "\n",
    "# Get the frequency bins corresponding to each time frame\n",
    "frequencies = librosa.core.fft_frequencies(sr=sr)\n",
    "\n",
    "# Find the frequency bin with the highest magnitude for each time frame\n",
    "dominant_frequency_bins = np.argmax(D, axis=0)\n",
    "\n",
    "# Convert frequency bins to Hertz\n",
    "dominant_frequencies = frequencies[dominant_frequency_bins]\n",
    "\n",
    "# Create a time axis\n",
    "times = librosa.times_like(dominant_frequencies)\n",
    "\n",
    "# Convert Hertz to note names and octaves with mapping to the closest note\n",
    "def frequency_to_note(frequency):\n",
    "    A4_frequency = 440.0\n",
    "    note_names = [\"C\", \"C#\", \"D\", \"D#\", \"E\", \"F\", \"F#\", \"G\", \"G#\", \"A\", \"A#\", \"B\"]\n",
    "    \n",
    "    try:\n",
    "        note_number = 12 * np.log2(frequency / A4_frequency)\n",
    "        octave = int(note_number) // 12\n",
    "        note_index = round(int(note_number) % 12)  # Round to the nearest note index\n",
    "        \n",
    "        # Handle cases where the calculated note index is outside the 0-11 range\n",
    "        if note_index < 0:\n",
    "            note_index = 0\n",
    "        elif note_index > 11:\n",
    "            note_index = 11\n",
    "        \n",
    "        return f\"{note_names[note_index]}{octave}\"\n",
    "    except (OverflowError, ZeroDivisionError):\n",
    "        return \"N/A\"  # Set a placeholder for problematic cases\n",
    "\n",
    "note_labels = [frequency_to_note(f) for f in dominant_frequencies]\n",
    "\n",
    "# Create a color map for octaves using seaborn\n",
    "octave_colors = sns.color_palette(\"husl\", n_colors=len(set(note_labels)))\n",
    "\n",
    "# Display the spectrogram with colored note labels\n",
    "plt.figure(figsize=(12, 6))\n",
    "librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='log')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Spectrogram with Colored Note Labels')\n",
    "\n",
    "# Add note labels to the frequency axis with colors based on octaves\n",
    "for i, label in enumerate(note_labels):\n",
    "    plt.text(-0.5, i, label, fontsize=8, ha=\"right\", va=\"center\", color=octave_colors[i])\n",
    "\n",
    "# Set the color map for the y-axis to match octave colors\n",
    "plt.yticks(range(0, len(frequencies), 12), ['C0', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8'])\n",
    "plt.gca().yaxis.set_tick_params(labelcolor=octave_colors)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the audio file\n",
    "audio_file = \"data/audio/nenjinile1-10.wav\"\n",
    "y, sr = librosa.load(audio_file)\n",
    "\n",
    "# Compute the spectrogram\n",
    "D = np.abs(librosa.stft(y))\n",
    "\n",
    "# Get the frequency bins corresponding to each time frame\n",
    "frequencies = librosa.core.fft_frequencies(sr=sr)\n",
    "\n",
    "# Find the frequency bin with the highest magnitude for each time frame\n",
    "dominant_frequency_bins = np.argmax(D, axis=0)\n",
    "\n",
    "# Convert frequency bins to Hertz\n",
    "dominant_frequencies = frequencies[dominant_frequency_bins]\n",
    "\n",
    "# Create a time axis\n",
    "times = librosa.times_like(dominant_frequencies)\n",
    "\n",
    "# Convert Hertz to note names and octaves with mapping to the closest note\n",
    "def frequency_to_note(frequency):\n",
    "    A4_frequency = 440.0\n",
    "    note_names = [\"C\", \"C#\", \"D\", \"D#\", \"E\", \"F\", \"F#\", \"G\", \"G#\", \"A\", \"A#\", \"B\"]\n",
    "    \n",
    "    try:\n",
    "        note_number = 12 * np.log2(frequency / A4_frequency)\n",
    "        octave = int(note_number) // 12\n",
    "        note_index = round(int(note_number) % 12)  # Round to the nearest note index\n",
    "        \n",
    "        # Handle cases where the calculated note index is outside the 0-11 range\n",
    "        if note_index < 0:\n",
    "            note_index = 0\n",
    "        elif note_index > 11:\n",
    "            note_index = 11\n",
    "        \n",
    "        return f\"{note_names[note_index]}{octave}\"\n",
    "    except (OverflowError, ZeroDivisionError):\n",
    "        return \"N/A\"  # Set a placeholder for problematic cases\n",
    "\n",
    "note_labels = [frequency_to_note(f) for f in dominant_frequencies]\n",
    "\n",
    "# Create a color map for octaves using seaborn\n",
    "unique_note_labels = list(set(note_labels))\n",
    "octave_colors = sns.color_palette(\"husl\", n_colors=len(unique_note_labels))\n",
    "\n",
    "# Display the spectrogram with colored note labels\n",
    "plt.figure(figsize=(12, 6))\n",
    "librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='log')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Spectrogram with Colored Note Labels')\n",
    "\n",
    "# Add note labels to the frequency axis with colors based on octaves\n",
    "for i, label in enumerate(note_labels):\n",
    "    color_index = unique_note_labels.index(label)  # Find the index of the label in unique_note_labels\n",
    "    plt.text(-0.5, i, label, fontsize=8, ha=\"right\", va=\"center\", color=octave_colors[color_index])\n",
    "\n",
    "# Set the color map for the y-axis to match octave colors\n",
    "plt.yticks(range(0, len(frequencies), 12), ['C0', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8'])\n",
    "plt.gca().yaxis.set_tick_params(labelcolor=octave_colors[0:len(frequencies):12])  # Adjust for octave colors\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the audio file with the correct file path\n",
    "audio_file = \"data/audio/nenjinile1-10.wav\"  # Update the file path as needed\n",
    "y, sr = librosa.load(audio_file)\n",
    "\n",
    "# Compute the spectrogram\n",
    "D = np.abs(librosa.stft(y))\n",
    "\n",
    "# Get the frequency bins corresponding to each time frame\n",
    "frequencies = librosa.core.fft_frequencies(sr=sr)\n",
    "\n",
    "# Find the frequency bin with the highest magnitude for each time frame\n",
    "dominant_frequency_bins = np.argmax(D, axis=0)\n",
    "\n",
    "# Convert frequency bins to Hertz\n",
    "dominant_frequencies = frequencies[dominant_frequency_bins]\n",
    "\n",
    "# Create a time axis\n",
    "times = librosa.times_like(dominant_frequencies)\n",
    "\n",
    "# Convert Hertz to note names without octaves\n",
    "def frequency_to_note(frequency):\n",
    "    note_names = [\"C\", \"C#\", \"D\", \"D#\", \"E\", \"F\", \"F#\", \"G\", \"G#\", \"A\", \"A#\", \"B\"]\n",
    "    \n",
    "    try:\n",
    "        note_index = round(12 * np.log2(frequency / 440.0)) % 12  # Calculate note index\n",
    "        return note_names[note_index]\n",
    "    except (OverflowError, ZeroDivisionError):\n",
    "        return \"N/A\"  # Set a placeholder for problematic cases\n",
    "\n",
    "note_labels = [frequency_to_note(f) for f in dominant_frequencies]\n",
    "\n",
    "# Create a color map for note labels using seaborn\n",
    "unique_note_labels = list(set(note_labels))\n",
    "note_colors = sns.color_palette(\"husl\", n_colors=len(unique_note_labels))\n",
    "\n",
    "# Display the spectrogram with colored note labels\n",
    "plt.figure(figsize=(12, 6))\n",
    "librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='log')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Spectrogram with Colored Note Labels')\n",
    "\n",
    "# Add note labels to the frequency axis with colors\n",
    "for i, label in enumerate(note_labels):\n",
    "    color_index = unique_note_labels.index(label)  # Find the index of the label in unique_note_labels\n",
    "    plt.text(-0.5, i, label, fontsize=8, ha=\"right\", va=\"center\", color=note_colors[color_index])\n",
    "\n",
    "plt.ylim(80, 600)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy.signal import butter, lfilter\n",
    "\n",
    "# Load the audio file with the correct file path\n",
    "audio_file = \"data/audio/nenjinile1-10.wav\"  # Update the file path as needed\n",
    "y, sr = librosa.load(audio_file)\n",
    "# Define the desired frequency range for voice (80 Hz to 2000 Hz)\n",
    "lowcut = 80.0  # Lower cutoff frequency in Hz\n",
    "highcut = 2000.0  # Upper cutoff frequency in Hz\n",
    "\n",
    "# Apply a bandpass filter to the audio signal\n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyquist = 0.5 * fs\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    y_filtered = lfilter(b, a, y)\n",
    "    return y_filtered\n",
    "\n",
    "# Apply the bandpass filter to the audio\n",
    "y_f = butter_bandpass(lowcut, highcut, sr)\n",
    "\n",
    "# Compute the spectrogram\n",
    "D = np.abs(librosa.stft(y_f))\n",
    "\n",
    "# Get the frequency bins corresponding to each time frame\n",
    "frequencies = librosa.core.fft_frequencies(sr=sr)\n",
    "\n",
    "# Find the frequency bin with the highest magnitude for each time frame\n",
    "dominant_frequency_bins = np.argmax(D, axis=0)\n",
    "\n",
    "# Convert frequency bins to Hertz\n",
    "dominant_frequencies = frequencies[dominant_frequency_bins]\n",
    "\n",
    "# Create a time axis\n",
    "times = librosa.times_like(dominant_frequencies)\n",
    "\n",
    "# Convert Hertz to note names without octaves\n",
    "def frequency_to_note(frequency):\n",
    "    note_names = [\"C\", \"C#\", \"D\", \"D#\", \"E\", \"F\", \"F#\", \"G\", \"G#\", \"A\", \"A#\", \"B\"]\n",
    "    \n",
    "    try:\n",
    "        note_index = round(12 * np.log2(frequency / 440.0)) % 12  # Calculate note index\n",
    "        return note_names[note_index]\n",
    "    except (OverflowError, ZeroDivisionError):\n",
    "        return \"N/A\"  # Set a placeholder for problematic cases\n",
    "\n",
    "note_labels = [frequency_to_note(f) for f in dominant_frequencies]\n",
    "\n",
    "# Create a color map for note labels using seaborn\n",
    "unique_note_labels = list(set(note_labels))\n",
    "note_colors = sns.color_palette(\"husl\", n_colors=len(unique_note_labels))\n",
    "\n",
    "# Display the spectrogram with colored note labels\n",
    "plt.figure(figsize=(12, 6))\n",
    "librosa.display.specshow(\n",
    "    librosa.amplitude_to_db(D, ref=np.max), sr=sr, x_axis='time', y_axis='hz', cmap='viridis'\n",
    ")\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Spectrogram with Colored Note Labels')\n",
    "\n",
    "# Add note labels to the frequency axis with colors\n",
    "for i, label in enumerate(note_labels):\n",
    "    color_index = unique_note_labels.index(label)  # Find the index of the label in unique_note_labels\n",
    "    plt.text(-0.5, i, label, fontsize=8, ha=\"right\", va=\"center\", color=note_colors[color_index])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the audio file using librosa\n",
    "audio_file = \"data/audio/nenjinile1-10.wav\"  # Update the file path as needed\n",
    "y, sr = librosa.load(audio_file, duration=40.0)  # Load the first 40 seconds\n",
    "\n",
    "# Spectrogram using 'spectrogram' command\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.subplot(2, 1, 1)\n",
    "S = librosa.stft(y, n_fft=5000, hop_length=240, win_length=5000)\n",
    "librosa.display.specshow(librosa.amplitude_to_db(S, ref=np.max), sr=sr, x_axis='time', y_axis='hz')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Spectrogram using spectrogram')\n",
    "\n",
    "# Spectrogram using short-time Fourier transform 'stft'\n",
    "plt.subplot(2, 1, 2)\n",
    "wlen = 5000  # Window length\n",
    "h = 400  # Overlap is wlen - h\n",
    "S = librosa.stft(y, n_fft=wlen, hop_length=h, win_length=wlen, center=False)\n",
    "librosa.display.specshow(np.log10(np.abs(S)), sr=sr, x_axis='time', y_axis='hz')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Spectrogram using stft')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import scipy.signal as signal\n",
    "import soundfile as sf\n",
    "\n",
    "# Load the audio file using librosa\n",
    "audio_file = \"data/audio/nenjinile1-10.wav\"  # Update the file path as needed\n",
    "y, sr = librosa.load(audio_file, duration=40.0, mono=False)  # Load the first 40 seconds in stereo\n",
    "\n",
    "# Define the frequency range for the bandpass filter (200 Hz to 600 Hz)\n",
    "lowcut = 200  # Low cutoff frequency in Hz\n",
    "highcut = 600  # High cutoff frequency in Hz\n",
    "\n",
    "# Design a bandpass filter\n",
    "nyquist = 0.5 * sr\n",
    "low = lowcut / nyquist\n",
    "high = highcut / nyquist\n",
    "b, a = signal.butter(6, [low, high], btype='band')\n",
    "\n",
    "# Apply the bandpass filter to each channel of the stereo audio\n",
    "filtered_left = signal.lfilter(b, a, y[0, :])\n",
    "filtered_right = signal.lfilter(b, a, y[1, :])\n",
    "\n",
    "# Combine the filtered left and right channels into a stereo signal\n",
    "filtered_audio = np.vstack((filtered_left, filtered_right))\n",
    "\n",
    "# Save the filtered stereo audio to a new file\n",
    "filtered_audio_file = \"data/audio/FS_nenjinile1-10.wav\"  # Update the file name as needed\n",
    "sf.write(filtered_audio_file, filtered_audio.T, sr)\n",
    "\n",
    "print(f\"Filtered stereo audio saved to {filtered_audio_file}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gMap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
